{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from OpenFabLibrary import JeibaCutWords\n",
    "from OpenFabLibrary import AppendKeywordCheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEAGAL_CLASS = 0\n",
    "VIOLATE_CLASS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 讀取training set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/\".join((\".\", \"data\"))\n",
    "data_source = \"train.csv\"\n",
    "data_df = pd.read_csv(open(data_dir + '/' + data_source, 'r', encoding='utf8'), delimiter=',')\n",
    "print(\"違法廣告: %d則\" % (data_df[data_df[\"Class\"] == 1].shape[0]))\n",
    "print(\"合法廣告: %d則\" % (data_df[data_df[\"Class\"] == 0].shape[0]))\n",
    "print(data_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 斷詞方法選擇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 斷詞處理\n",
    "train_data_df = JeibaCutWords(data_df)\n",
    "print(train_data_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 關鍵字檢查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 關鍵字檢查\n",
    "train_data_df['keyword_flag'], _ = AppendKeywordCheck(train_data_df)\n",
    "print(train_data_df.head(5))\n",
    "\n",
    "# 儲存為pickle格式檔案\n",
    "with open(data_dir + '/' + 'train_tokenized', 'wb') as file:\n",
    "    pickle.dump(train_data_df, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 製作文字雲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from OpenFabLibrary import ShowWordCloud\n",
    "\n",
    "with open(data_dir + '/' + 'train_tokenized', 'rb') as file:\n",
    "    train_tokenized_df = pickle.load(file)\n",
    "\n",
    "ShowWordCloud(train_tokenized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec轉換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + '/' + 'train_tokenized', 'rb') as file:\n",
    "    train_tokenized_df = pickle.load(file)\n",
    "\n",
    "corpus_source = train_tokenized_df['sentence']\n",
    "print(corpus_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 設定Word2Vec參數並訓練詞向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD2VEC_DIMENTION = 128\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "\n",
    "def TrainWord2VecModel(input_corpus):\n",
    "    # build word2vec\n",
    "    # sg=0 CBOW ; sg=1 skip-gram\n",
    "    model = word2vec.Word2Vec(size=WORD2VEC_DIMENTION, min_count=5, window=5, sg=0)\n",
    "\n",
    "    # build vocabulary\n",
    "    model.build_vocab(input_corpus)\n",
    "\n",
    "    # train word2vec model ; shuffle data every epoch\n",
    "    for i in range(20):\n",
    "        print(\"%d-th training\" % (i))\n",
    "        random.shuffle(input_corpus)\n",
    "        model.train(input_corpus, total_examples=len(input_corpus), epochs=1)\n",
    "\n",
    "    ## save model\n",
    "    model.save('word2vec_model/CBOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 若要用已經訓練好的詞向量，這一步可以不做\n",
    "# 若要訓練新的詞向量，把註解拿掉\n",
    "# 現階段使用pre-trained mode zh, 300d, 50101 words\n",
    "TrainWord2VecModel(corpus_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = word2vec.Word2Vec.load('word2vec_model/CBOW')  # 載入剛剛訓練好的Word2Vec model\n",
    "print(\" \\\"%s\\\" 字詞相似度: \" % ('改善'))\n",
    "#print(w2v.wv['改善'])\n",
    "w2v.wv.most_similar('改善')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 列印出訓練好的詞向量\n",
    "print(\"詞向量維度:\", w2v.wv.vectors.shape)\n",
    "fo = open(\"./word2vec.txt\", \"w+\")\n",
    "for index, vector in enumerate(w2v.wv.vectors):\n",
    "    #print(vector)\n",
    "    fo.write(str(vector))\n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import word2vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/\".join((\".\", \"data\"))\n",
    "train_data_source = 'train_tokenized'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset and sentence to sequence transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load article tokenized\n",
    "with open(data_dir + \"/\" + train_data_source, 'rb') as file:\n",
    "    article_df = pickle.load(file)\n",
    "article_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"違法廣告: %d則\" % (article_df[article_df[\"class\"] == 1].shape[0]))\n",
    "print(\"合法廣告: %d則\" % (article_df[article_df[\"class\"] == 0].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create word ID mapping and word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v = word2vec.Word2Vec.load('word2vec_model/CBOW')\n",
    "w2v = word2vec.Word2Vec.load('word2vec_model/zh.bin')\n",
    "word2id = {k:i for i, k in enumerate(w2v.wv.vocab.keys())}\n",
    "id2word = {i:k for k, i in word2id.items()}\n",
    "word2id_len = len(word2id) - 1\n",
    "print('word2id_len:', word2id_len)\n",
    "\n",
    "#WORD2VEC_DIMENTION = 128\n",
    "WORD2VEC_DIMENTION = 300\n",
    "embedding = np.zeros((word2id_len+2, WORD2VEC_DIMENTION))\n",
    "for k, v in word2id.items():\n",
    "    embedding[v] = w2v.wv[k]\n",
    "    # 謹慎列印，資料量很大\n",
    "    #print('k=%s, v=%d'%(k, v))\n",
    "    #print('embedding[v]=', embedding[v])\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence to sequence transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# 選取多少詞來當作輸入\n",
    "#\n",
    "PICK_WORDS = 40  # 選前面40個詞當作輸入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_id = []\n",
    "\n",
    "for setence in article_df[\"sentence\"]:\n",
    "    text = setence[:PICK_WORDS]\n",
    "    #print(text)\n",
    "    ids = [word2id_len+1]*PICK_WORDS  # 初始化list\n",
    "    \n",
    "    #for w in text:\n",
    "    #    if w in word2id:\n",
    "    #        ids[:len(text)] = word2id[w]\n",
    "    #    else\n",
    "    #        ids[:len(text)] = word2id_len+1\n",
    "    ids[:len(text)] = [word2id[w] if w in word2id else word2id_len+1 for w in text]\n",
    "    print(ids)\n",
    "\n",
    "    docs_id.append(ids)\n",
    "\n",
    "#  轉換後的sequence合併到dataframe    \n",
    "article_df[\"sentence_seq\"] = docs_id\n",
    "article_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_docs_id = np.array(docs_id)\n",
    "print(a_docs_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number_of_classes = len(category_list)\n",
    "number_of_classes = 2  # 合法、違法廣告\n",
    "number_of_classes_binary = 1\n",
    "sample_per_class  = 8\n",
    "\n",
    "epochs            = 500  #100\n",
    "batch_size        = number_of_classes * sample_per_class\n",
    "update_per_epochs = 100  #100\n",
    "hidden_layer_size = 32 #64 #256\n",
    "number_of_layers  = 2\n",
    "learning_rate     = 0.001  #0.001\n",
    "#dropout           = False\n",
    "dropout_rate      = 0.5\n",
    "wv                = embedding\n",
    "gradient_clip_margin = 4\n",
    "\n",
    "patience = 5  # early stop patiences\n",
    "n_patience = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_generator(df, bz):\n",
    "    # bz: batch size \n",
    "    dfs = [sub_df for key, sub_df in df.groupby('class')]\n",
    "    df_n = len(dfs)\n",
    "    \n",
    "    while True:\n",
    "        selected = pd.concat([sub_df.sample(int(bz/number_of_classes)) for sub_df in dfs], axis=0)\n",
    "        selected = selected.sample(frac=1)\n",
    "        #print(\"selected: \", selected)\n",
    "        x = selected['sentence_seq'].tolist()\n",
    "        x = np.array(x)\n",
    "        #y = selected.as_matrix(columns=['class'])  # kvdbg+ # pandas for elder version \n",
    "        y = selected[[\"class\"]].values  # kvdbg+ # pandas for new version \n",
    "        #y = pd.get_dummies(selected['class'], '').as_matrix()  # one-hot encoding\n",
    "        #print(y)\n",
    "        \n",
    "        yield x, y\n",
    "        \n",
    "def test_data_generator(df, docs_id):\n",
    "    #print(df)\n",
    "    docs_id = np.array(docs_id)\n",
    "    xx = docs_id[df.index]\n",
    "    x = df['sentence_seq'].tolist()\n",
    "    x = np.array(x)\n",
    "    #print(df[[\"sentence\",\"sentence_seq\"]])\n",
    "    #y = df.as_matrix(columns=['class']) # pandas for elder version \n",
    "    y = df[[\"class\"]].values # pandas for new version\n",
    "    #kvdbg-y = df['class'].as_matrix()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 切割資料準備訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validate_df = train_test_split(article_df, test_size=0.2, shuffle=True, stratify=article_df['class'])\n",
    "\n",
    "train_generate = train_data_generator(train_df, batch_size)\n",
    "X_test, y_test = test_data_generator(validate_df, docs_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_loss(logits, targets, learning_rate, grad_clip_margin):\n",
    "    #loss = tf.reduce_sum(tf.pow(logits - targets, 2))/batch_size\n",
    "    #kvdbg-cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=targets, logits=logits))\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=targets, logits=logits)) #kvdbg+\n",
    "     \n",
    "\n",
    "    #Cliping the gradient loss\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    gradients = optimizer.compute_gradients(cross_entropy)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, (-1)*grad_clip_margin, grad_clip_margin), var) for grad, var in gradients if grad is not None]\n",
    "    train_optimizer = optimizer.apply_gradients(capped_gradients)   \n",
    "    #curr_learning_rate = (optimizer._lr_t * tf.sqrt(1 - optimizer._beta1) / (1 - optimizer._beta2))\n",
    "    curr_learning_rate = optimizer._lr_t\n",
    "    \n",
    "\n",
    "    return cross_entropy, train_optimizer, curr_learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_graph = tf.Graph()\n",
    "\n",
    "with main_graph.as_default():    \n",
    "    ##defining placeholders##\n",
    "    with tf.name_scope('input_layer'):\n",
    "        inputs = tf.placeholder(tf.int32, [None, PICK_WORDS], name='input_data')\n",
    "        tf.add_to_collection(\"training_collection\", inputs)  # 把這個變數存起來\n",
    "        \n",
    "        targets = tf.placeholder(tf.float32, [None, number_of_classes_binary], name='targets')\n",
    "        tf.add_to_collection(\"training_collection\", inputs)  # 把這個變數存起來\n",
    "        \n",
    "        bz = tf.placeholder(tf.int32, [], name='batch_size')\n",
    "        \n",
    "        keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "        tf.add_to_collection(\"training_collection\", inputs)  # 把這個變數存起來\n",
    "        \n",
    "    ## embedding lookup table\n",
    "    with tf.variable_scope('embedding_layer'):    \n",
    "        em_W = tf.Variable(wv.astype(np.float32), trainable=True)  #wv.shape = (sentences_count, word2vec_dimension)\n",
    "        x = tf.nn.embedding_lookup(em_W, inputs)    #x.shape = (?, PICK_WORDS, word2vec_dimension)\n",
    "        \n",
    "    ##LSTM layer##\n",
    "    ##Bi-directional LSTM\n",
    "    with tf.variable_scope(\"Bidirectional_LSTM_layer\"):\n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer_size)\n",
    "        #if dropout:\n",
    "        lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\n",
    "            \n",
    "        init_state_fw = lstm_cell.zero_state(tf.shape(inputs)[0], tf.float32)\n",
    "        init_state_bw = lstm_cell.zero_state(tf.shape(inputs)[0], tf.float32)\n",
    "        \n",
    "        ((outputs_fw, outputs_bw), (outputs_state_fw, outputs_state_bw)) = \\\n",
    "        tf.nn.bidirectional_dynamic_rnn(lstm_cell, lstm_cell, x, \n",
    "                                        initial_state_fw=init_state_fw,\n",
    "                                        initial_state_bw=init_state_bw)\n",
    "        \n",
    "        outputs = tf.concat((outputs_fw, outputs_bw), 2)\n",
    "        print(outputs)\n",
    "        #final_state_c = tf.concat((outputs_state_fw.c, outputs_state_bw.c), 1)\n",
    "        #final_state_h = tf.concat((outputs_state_fw.h, outputs_state_bw.h), 1)\n",
    "        #outputs = tf.contrib.rnn.LSTMStateTuple(c=final_state_c, h=final_state_h)\n",
    "\n",
    "    ##Output layer##   \n",
    "    with tf.variable_scope('output_layer'):\n",
    "        x = outputs[:, -1, :] \n",
    "        logits = tf.layers.dense(inputs=x, units=number_of_classes_binary, activation=None,\n",
    "                                 kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.005),)\n",
    "        tf.add_to_collection(\"training_collection\", logits)  # 把這個變數存起來\n",
    "        \n",
    "        class_prob = tf.nn.sigmoid(logits, name='class_probability')\n",
    "        tf.add_to_collection(\"training_collection\", class_prob)  # 把這個變數存起來\n",
    "        \n",
    "    ##loss and optimization##\n",
    "    with tf.name_scope('loss_and_opt'):\n",
    "        loss, opt, curr_lr = opt_loss(logits, targets, learning_rate, gradient_clip_margin)\n",
    "    \n",
    "    ##accuracy\n",
    "    with tf.name_scope('evaluate'):\n",
    "        predictions = tf.greater(class_prob, 0.5, name=\"predictions\")\n",
    "        tf.add_to_collection(\"training_collection\", inputs)  # 把這個變數存起來\n",
    "        \n",
    "        correct_prediction = tf.equal(tf.round(class_prob), targets)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # 建立 saver 物件\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess = tf.Session(graph=main_graph)\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    traind_loss = []\n",
    "    traind_scores = []\n",
    "    test_scores = []\n",
    "\n",
    "    for j in range(update_per_epochs):\n",
    "        X_batch, y_batch = next(train_generate) \n",
    "        \n",
    "        train_logits, train_prob, train_loss, train_acc, optimizer = \\\n",
    "        sess.run([logits, class_prob, loss, accuracy, opt], \n",
    "                 feed_dict={inputs:X_batch,\n",
    "                            targets:y_batch,\n",
    "                            bz:np.array(batch_size),\n",
    "                            keep_prob:(1 - dropout_rate)})\n",
    "        \n",
    "        traind_loss.append(train_loss)\n",
    "        traind_scores.append(train_acc)\n",
    "        \n",
    "    #kvdbg- y_test_onehot = pd.get_dummies(y_test, '')  # one-hot encoding\n",
    "    #print('y_test:', y_test)\n",
    "    \n",
    "    test_logits, test_prob, test_loss, test_acc = \\\n",
    "    sess.run([logits, class_prob, loss, accuracy], \n",
    "             #kvdbg-feed_dict={inputs:X_test, targets:y_test_onehot, bz:np.array(len(X_test))})\n",
    "             feed_dict={inputs:X_test, \n",
    "                        targets:y_test, \n",
    "                        bz:np.array(len(X_test)),\n",
    "                        keep_prob:(1 - dropout_rate)})\n",
    "    \n",
    "    #print(\"test_prob: \", test_prob)\n",
    "    \n",
    "    train_loss_list.append(np.mean(traind_loss))\n",
    "    train_acc_list.append(np.mean(traind_scores))\n",
    "    \n",
    "    test_loss_list.append(test_loss)\n",
    "    test_acc_list.append(test_acc)\n",
    "    \n",
    "    if (i % 1) == 0:\n",
    "        print('Epoch {}/{}'.format(i, epochs), \n",
    "              ' Train loss: {:.3f}'.format(np.mean(traind_loss)),'Train acc: {:.3f}'.format(np.mean(traind_scores)),\n",
    "              ' Test loss: {:.3f}'.format(test_loss), ' Test acc: {:.3f}'.format(test_acc))\n",
    "        \n",
    "    if test_loss >= np.min(test_loss_list):\n",
    "        n_patience += 1\n",
    "    else:\n",
    "        n_patience = 0\n",
    "\n",
    "    if n_patience > patience:\n",
    "        print(\"The model didn't improve for %i rounds, break it!\" % patience)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loss')\n",
    "plt.plot(np.arange(len(train_loss_list)), train_loss_list, 'b', label = 'train')\n",
    "plt.plot(np.arange(len(test_loss_list)), test_loss_list, 'r', label = 'test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('accuracy')\n",
    "plt.plot(np.arange(len(train_acc_list)), train_acc_list, 'b', label = 'train')\n",
    "plt.plot(np.arange(len(test_acc_list)), test_acc_list, 'r', label = 'test')\n",
    "plt.legend(loc = 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = saver.save(sess, \"./model/lstm_model\") # 儲存模型到 /tmp/model.ckpt\n",
    "print(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    #初始化variable\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #擷取最新的checkpoint\n",
    "    latest_ckpt = tf.train.latest_checkpoint('./model/')\n",
    "\n",
    "    #載入graph\n",
    "    restore_saver = tf.train.import_meta_graph('./model/lstm_model.meta')\n",
    "\n",
    "    #恢复图，即将weights等参数加入图对应位置中\n",
    "    restore_saver.restore(sess, latest_ckpt)\n",
    "    #print(tf.get_collection('training_collection'))  # get_collection返回list，裡面存放訓練模型時候的變數\n",
    "        \n",
    "    #graph variable轉為常量\n",
    "    output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "        sess, sess.graph_def, [\"output_layer/class_probability\"] )\n",
    "    \n",
    "    #graph寫入pb file\n",
    "    #model_f = tf.gfile.GFile(\"./model/frozen_model.pb\",\"wb\")\n",
    "    #model_f.write(output_graph_def.SerializeToString())\n",
    "    tf.train.write_graph(output_graph_def, './model', 'frozen_model.pb',as_text=False)\n",
    "    print ('{} ops in the final graph.'.format(len(output_graph_def.node)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Run Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import word2vec\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from OpenFabLibrary import JeibaCutWords\n",
    "from OpenFabLibrary import AppendKeywordCheck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create word ID mapping and word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/\".join((\".\", \"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v = word2vec.Word2Vec.load('word2vec_model/CBOW')\n",
    "w2v = word2vec.Word2Vec.load('word2vec_model/zh.bin')\n",
    "word2id = {k:i for i, k in enumerate(w2v.wv.vocab.keys())}\n",
    "id2word = {i:k for k, i in word2id.items()}\n",
    "word2id_len = len(word2id) - 1\n",
    "print('word2id_len:', word2id_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  AI預測 + 關鍵字檢查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jieba_validation(input_text):\n",
    "    single_ad = 1  # 若是單一則廣告輸入，設 1\n",
    "               # 若是一大批廣告輸入，設 0\n",
    "        \n",
    "    ad_ID = 0\n",
    "    ad_Name = \"測試產品\"\n",
    "    ad_Class = 0\n",
    "\n",
    "    ad_Description = input_text\n",
    "    \n",
    "    if single_ad:\n",
    "        # 單一廣告輸入\n",
    "        test_data_df = pd.DataFrame({'ID': [ad_ID], \n",
    "                                     'Name':[ad_Name],\n",
    "                                     'Description':[ad_Description],\n",
    "                                     'Class':[ad_Class]})\n",
    "    else:\n",
    "        # 大批廣告輸入\n",
    "        test_data_source = \"test_private.csv\"\n",
    "        test_data_df = pd.read_csv(open(data_dir + '/' + test_data_source, 'r', encoding='utf8'), delimiter=',')\n",
    "\n",
    "\n",
    "    # 斷詞處理\n",
    "    test_df = JeibaCutWords(test_data_df)\n",
    "\n",
    "    # 關鍵字檢查\n",
    "    test_df['keyword_flag'], keywords_list = AppendKeywordCheck(test_df)\n",
    "    \n",
    "    #\n",
    "    # 選取多少詞來當作輸入\n",
    "    #\n",
    "    PICK_WORDS = 40  # 選前面40個詞當作輸入，這個長度要跟訓練模型的長度一樣\n",
    "    batch_size = 16  # 若是資料筆數很多，一次讀batch_size筆資料來預測\n",
    "\n",
    "    docs_pred_id = []\n",
    "    for doc in test_df['sentence']:\n",
    "        text = doc[:PICK_WORDS]\n",
    "        ids = [word2id_len+1]*PICK_WORDS\n",
    "        ids[:len(text)] = [word2id[w] if w in word2id else word2id_len+1 for w in text]\n",
    "        docs_pred_id.append(ids)\n",
    "\n",
    "    # 轉換後的sequence合併到dataframe    \n",
    "    test_df['sentence_seq'] = docs_pred_id\n",
    "\n",
    "    x = test_df['sentence_seq'].tolist()\n",
    "    X_pred = np.array(x)\n",
    "    #y_actual = test_df['class'].as_matrix() # pandas for elder version \n",
    "    #y_keyword_flag = test_df['keyword_flag'].as_matrix() # pandas for elder version \n",
    "    y_actual = test_df['class'].values # pandas for new version \n",
    "    y_keyword_flag = test_df['keyword_flag'].values # pandas for new version \n",
    "    \n",
    "    #\n",
    "    # Load trained model and feed data to predict\n",
    "    #\n",
    "    pred_input = X_pred\n",
    "    pred_batch_size = batch_size\n",
    "    output_class = []\n",
    "    output_probability = []\n",
    "\n",
    "    with tf.gfile.GFile(\"./model/frozen_model.pb\", \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        # The name var will prefix every op/nodes in your graph\n",
    "        # Since we load everything in a new graph, this is not needed\n",
    "        tf.import_graph_def(graph_def, name=\"prefix\")\n",
    "        \n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        #saver = tf.train.import_meta_graph('./model/lstm_model.meta')\n",
    "        #saver.restore(sess, tf.train.latest_checkpoint('./model/'))\n",
    "        #graph = tf.get_default_graph()\n",
    "            \n",
    "        inputs = graph.get_tensor_by_name('prefix/input_layer/input_data:0')\n",
    "        keep_prob = graph.get_tensor_by_name('prefix/input_layer/keep_prob:0')\n",
    "        class_prob = graph.get_tensor_by_name('prefix/output_layer/class_probability:0')\n",
    "        #predict_out = graph.get_tensor_by_name('prefix/evaluate/predictions:0')\n",
    "        \n",
    "        for start in range(0, len(pred_input), pred_batch_size):\n",
    "            end = min(start + batch_size, len(pred_input))\n",
    "\n",
    "            x_pred_batch = pred_input[start:end]        \n",
    "\n",
    "            if np.ndim(x_pred_batch)==1:\n",
    "                x_pred_batch = x_pred_batch.reshape([1,-1])\n",
    "\n",
    "            #\n",
    "            # 把剛剛載入的模型拿來用\n",
    "            #\n",
    "            #pred_result, pred_prob = sess.run([predict_out, class_prob],\n",
    "            #                                  feed_dict = {inputs:x_pred_batch})\n",
    "            pred_prob = sess.run([class_prob], feed_dict = {inputs:x_pred_batch, keep_prob:1})\n",
    "            pred_result = np.around(pred_prob)  #四捨五入，機率 > 0.5，視為class \"1\"\n",
    "\n",
    "            output_class.extend(pred_result)\n",
    "            output_probability.extend(pred_prob)\n",
    "\n",
    "    # 預測的類別\n",
    "    y_pred_class = output_class\n",
    "    \n",
    "\n",
    "    # 預測的類別機率值\n",
    "    #kvdbg-Legal_prob = output_probability[:,0]    # column[0]是class 0的機率\n",
    "    #kvdbg-Violate_prob = output_probability[:,1]  # column[1]是class 1的機率\n",
    "    \n",
    "    if single_ad:\n",
    "        # 單一廣告判別\n",
    "        if y_pred_class[0] == 0:\n",
    "            keywords_list = []  # 合法廣告不用列出違規關鍵字\n",
    "            return \"合法\", output_probability, keywords_list\n",
    "        else:\n",
    "            return \"違法\", output_probability, keywords_list\n",
    "    else:\n",
    "        # 大批廣告判別\n",
    "        return y_pred_class, output_probability, keywords_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入測試資料集，並進行預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 單一廣告\n",
    "ad_text = \"含500億活菌數及八種益生菌，排便不順，氣味難聞，當心健康拉警報\\\n",
    "服用本產品可達到體內環保、增強抵抗力並強化細胞功能，可改善體質、促進新陳代謝、幫助維持消化道機能、促進食慾、開胃，促進腸道蠕動改變細菌叢生態，使排便順暢。\\\n",
    "\"\n",
    "\n",
    "result, probability, keywords = jieba_validation(ad_text)\n",
    "\n",
    "print(\"辨識結果: \", result)\n",
    "print(\"違規機率: \", probability)\n",
    "print(\"違規字詞: \", keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
